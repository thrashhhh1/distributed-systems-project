version: '3.8'

services:
  app:
    build:
      context: ./nestjs-app
    container_name: distributedsystems
    ports:
      - "3000:3000"
    restart: unless-stopped
    environment:
      PORT: ${PORT:-3000}
      MONGODB_URL: ${MONGODB_URL:-mongodb://mongodb:27017/mongo-distributedsystems}
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      CACHE_TTL: ${CACHE_TTL:-60}
      TRAFFIC_DISTRIBUTION_TYPES: ${TRAFFIC_DISTRIBUTION_TYPES:-poisson,uniforme}
      # Añade la variable de entorno para Elasticsearch
      ELASTICSEARCH_NODE: http://elasticsearch:9200
      # Añade la variable para el directorio de salida de Pig
      PIG_OUTPUT_DIR: /app_results
    depends_on:
      - mongodb
      - redis
      - elasticsearch # Añadido para asegurar el orden de inicio
    volumes:
      - ./nestjs-app:/usr/src/app
      - /usr/src/app/node_modules
      # Montamos el directorio de resultados para que NestJS pueda leerlo
      - ./results:/app_results
    command: npm run start:dev
    networks: # <--- AÑADIDO
      - hadoop-network

  mongodb:
    image: mongo:5
    container_name: mongo-distributedsystems
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    restart: always
    networks: # <--- AÑADIDO
      - hadoop-network

  redis:
    image: redis:latest
    container_name: redis-distributedsystems
    ports:
      - "6379:6379"
    restart: always
    command: redis-server --save "" --appendonly no --maxmemory 250kb --maxmemory-policy allkeys-lru
    networks: # <--- AÑADIDO
      - hadoop-network

  # --- Servicios de Hadoop y Pig ---
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    hostname: namenode
    ports:
      - "9870:9870"
      - "8020:8020"
    env_file:
      - ./hadoop-config/hadoop.env
    environment:
      - CLUSTER_NAME=test
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      interval: 5s
      timeout: 2s
      retries: 10
    networks: # <--- AÑADIDO
      - hadoop-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    hostname: datanode
    env_file:
      - ./hadoop-config/hadoop.env
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "stat", "/hadoop/dfs/data/current/VERSION"]
      interval: 5s
      timeout: 2s
      retries: 10
    networks: # <--- AÑADIDO
      - hadoop-network

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    hostname: resourcemanager
    ports:
      - "8088:8088"
    env_file:
      - ./hadoop-config/hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://resourcemanager:8088"]
      interval: 5s
      timeout: 2s
      retries: 10
    networks: # <--- AÑADIDO
      - hadoop-network

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    hostname: nodemanager
    env_file:
      - ./hadoop-config/hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870,resourcemanager:8088"
    depends_on:
      namenode:
        condition: service_healthy
      resourcemanager:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://nodemanager:8042"]
      interval: 5s
      timeout: 2s
      retries: 10
    networks: # <--- AÑADIDO
      - hadoop-network

  pig:
    build:
      context: ./hadoop-env/pig-dockerfile
      dockerfile: Dockerfile
    hostname: pig
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
    depends_on:
      namenode: { condition: service_healthy }
      datanode: { condition: service_healthy }
      resourcemanager: { condition: service_healthy }
      nodemanager: { condition: service_healthy }
    volumes:
      - ./pig-scripts:/pig-scripts
      # El contenedor de NestJS ahora crea los datos en /usr/src/app/data
      # Montamos ese directorio para que Pig pueda accederlo.
      - ./nestjs-app/data:/waze_incidents
      - ./results:/app_results
    entrypoint: ["/usr/local/bin/load_data.sh"]
    restart: "no" # Cambiado a "no" para que solo se ejecute una vez
    networks:
      - hadoop-network

  # --- Servicios de ElasticSearch y Kibana ---
  elasticsearch:
    image: elasticsearch:8.9.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m" # Limitar memoria para evitar problemas
    ports:
      - "9200:9200"
    volumes:
      - elastic_data:/usr/share/elasticsearch/data
    restart: always
    networks: # <--- AÑADIDO
      - hadoop-network

  kibana:
    image: kibana:8.9.0
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    restart: always
    networks: # <--- AÑADIDO
      - hadoop-network

volumes:
  mongo_data:
  hadoop_namenode:
  hadoop_datanode:
  elastic_data: # <--- AÑADIDO para persistencia de Elasticsearch

networks:
  hadoop-network:
    driver: bridge
      


